{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis Sentimen Opini Masyarakat Terhadap Vaksinasi Nasional Menggunakan Metode Na√Øve Bayes dengan Seleksi Fitur TF-IDF (Bagian Naive Bayes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import library yang dibutuhkan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install sastrawi\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "id": "mWIWsNszC1hH",
    "outputId": "7e0c46ed-6329-4379-c5d9-2f4f55018ec5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from matplotlib import rc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter, defaultdict\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import csv\n",
    "import ast\n",
    "import matplotlib as mpl\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memuat data training\n",
    "Dalam Naive Bayes, kita kudu buat model dari Naive Bayesnya dulu (maksudnya kita kudu buat probabilitas dari tiap kata di tiap kelasnya). Jadi kita muat dulu train set-nya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "7r6bfrjmIHwU"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>['vaksin', 'rakyat', 'indonesia', 'mari', 'awa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>['vaksin', 'astrazeneca', 'rekomendasi', 'bpom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>['didik', 'kemenag', 'jombang', 'vaksinasi', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>['titer', 'antibodi', 'efektifitas', 'acu', 'b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>['vonis', 'mati', 'bpom', 'vaksin', 'nusantara...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              tweet\n",
       "0      1  ['vaksin', 'rakyat', 'indonesia', 'mari', 'awa...\n",
       "1      0  ['vaksin', 'astrazeneca', 'rekomendasi', 'bpom...\n",
       "2      1  ['didik', 'kemenag', 'jombang', 'vaksinasi', '...\n",
       "3      0  ['titer', 'antibodi', 'efektifitas', 'acu', 'b...\n",
       "4      0  ['vonis', 'mati', 'bpom', 'vaksin', 'nusantara..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"csv files/train_80.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memuat isi dari data training\n",
    "Karena kita udah melakukan preprocessing terhadap semua tweet, jadi kita tinggal memuat hasil tokenisasi yang ada di data training aja."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Ifov_XqaCBxI"
   },
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def tokenize(self, texts):\n",
    "        texts = ast.literal_eval(texts)\n",
    "        return [text for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes!\n",
    "Ini class dan function paling penting, karena didalam class ini terdapat function untuk mendapatkan hasil prediksi dengan menggunakan Naive Bayes. Inget tahapannya:\n",
    "- Ambil data training yang udah ngelewatin preprocessing\n",
    "- Ambil fitur-fitur yang dibutuhkan Naive Bayes berdasarkan perangkingan TF-IDF dengan persentase yang ditentukan (80%)\n",
    "- Tiap kata yang ada didalam tweet kita buat tabel kemunculannya (berdasarkan tweet dan sentimennya)\n",
    "- Hitung probabilitas dari tiap kata dalam tiap kelas dan probabilitas priornya\n",
    "- Prediksi data test dengan model yang udah dibuat!\n",
    "- Hitung Akurasi, Presisi, dan Recall/Sensitivity\n",
    "Btw, yang ada dibawah ini cuma class. Artinya kodingan dibawah ini cuma blueprint-nya. Kita akan \"run\" kodingannya nanti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "JnOAO_Kl4BkQ"
   },
   "outputs": [],
   "source": [
    "class MultinomialNaiveBayes:\n",
    "    def __init__(self, classes, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.classes = classes\n",
    "        \n",
    "    # Di function ini, kita pisahin tweet mana yang sentimennya negatif dan mana yang sentimennya positif  \n",
    "    def group_by_class(self, X, y):\n",
    "        data = dict()\n",
    "        for c in self.classes:\n",
    "            data[c] = X[np.where(y == c)]\n",
    "        return data\n",
    "    \n",
    "    # function ini untuk mendapatkan fitur yang udah didapetin dari hasil perangkingan TF-IDF\n",
    "    def get_features(self, features_percentage):\n",
    "        all_features = []\n",
    "        nb_features = []\n",
    "        with open(\"csv files/tfidf_rank_train80.csv\", newline='') as inputfile:\n",
    "            for row in csv.reader(inputfile):\n",
    "                all_features.append(row[0])\n",
    "        i = 0\n",
    "        while i < int(len(all_features)*features_percentage): \n",
    "            nb_features.append(all_features[i])\n",
    "            i = i + 1\n",
    "        return nb_features\n",
    "    \n",
    "    # Function ini untuk ngebuat model Naive Bayes, atau bisa dibilang untuk dapetin probabilitas dari tiap kata terhadap sentimennya\n",
    "    def fit(self, p, X, y):\n",
    "        self.n_class_items = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set(self.get_features(p))\n",
    "\n",
    "        n = len(X)\n",
    "        \n",
    "        grouped_data = self.group_by_class(X, y)\n",
    "        \n",
    "        for c, data in grouped_data.items():\n",
    "            self.n_class_items[c] = len(data)\n",
    "            self.log_class_priors[c] = math.log(self.n_class_items[c] / n)\n",
    "            self.word_counts[c] = defaultdict(lambda: 0)\n",
    "          \n",
    "            for text in data:\n",
    "                counts = Counter(self.tokenizer.tokenize(text))\n",
    "                for word, count in counts.items():\n",
    "                    if word in self.vocab:\n",
    "                        self.word_counts[c][word] += count\n",
    "        return self\n",
    "    \n",
    "    # Ini buat laplace smoothing, menambahkan satu ke pembilang\n",
    "    def laplace_smoothing(self, word, text_class):\n",
    "        num = self.word_counts[text_class][word] + 1\n",
    "        denom = self.n_class_items[text_class] + len(self.vocab)\n",
    "        return math.log(num / denom)\n",
    "    \n",
    "    # lalu ini buat prediksi\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        for text in X:  \n",
    "            class_scores = {c: self.log_class_priors[c] for c in self.classes}\n",
    "\n",
    "            words = set(self.tokenizer.tokenize(text))\n",
    "            for word in words:\n",
    "                if word not in self.vocab: continue\n",
    "                \n",
    "                for c in self.classes:\n",
    "                    log_w_given_c = self.laplace_smoothing(word, c)\n",
    "                    class_scores[c] += log_w_given_c\n",
    "                \n",
    "            result.append(max(class_scores, key=class_scores.get))\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## It's training time!\n",
    "Tadi kan aku udah bilang kalo kodingan diatas itu cuma blueprint aja. Karena function yang ada diatas itu butuh nilai parameter, jadi bisa dibilang kerjaan awal kita itu deklarasiin variabel untuk parameternya dulu. Baru deh abis itu kita prediksi sentimennya!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "kkuM0qhMNWfd"
   },
   "outputs": [],
   "source": [
    "# Ambil tweet dari yang ingin dimasukkan ke dalam fitur\n",
    "X_train = train['tweet'].values\n",
    "# Ambil nilai dari sentimennya\n",
    "y_train = train['label'].values\n",
    "# Mau menggunakan fiturnya berapa persen?\n",
    "fp = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ruhWLRfz4SlE"
   },
   "outputs": [],
   "source": [
    "# Hitung probabilitas dari tiap kata dalam tiap kelas dan hitung probabilitas prior tiap kelas\n",
    "MNB = MultinomialNaiveBayes(classes=np.unique(y_train), tokenizer=Tokenizer()).fit(fp, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing!\n",
    "Proses diatas kan buat modelnya tuh, artinya kita udah bisa ngitung sentimen analisisnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"csv files/test_20.csv\")\n",
    "X_test = test['tweet'].values\n",
    "y_test = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "5-aAAa3O5gKs"
   },
   "outputs": [],
   "source": [
    "y_pred = MNB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 0 0 0 1 1 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 1 0 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dfFrVVFP-sUn",
    "outputId": "7bb318e6-bc15-411b-8946-af0f0eaf8446"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9682539682539683"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "id": "83tQ3ERwRTgP",
    "outputId": "b1cd6b41-7bc4-45e6-ccd4-2acfa974ca76"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "djMXLvXdRVKd",
    "outputId": "c3d5d583-4347-4feb-8ca2-c5e5acc1e479"
   },
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "tkEbxgwORgOW",
    "outputId": "90200f02-a429-49a4-a86e-195f59b56a88"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "mpl.rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "class_names = [\"negative\", \"positive\"]\n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "\n",
    "sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\", fmt=\"d\", cbar=False, xticklabels=class_names, yticklabels=class_names)\n",
    "ax.xaxis.set_label_position('top')\n",
    "plt.tight_layout()\n",
    "plt.ylabel('Actual sentiment')\n",
    "plt.xlabel('Predicted sentiment');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5.sentiment_analysis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
