{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b5298f",
   "metadata": {},
   "source": [
    "# Analisis Sentimen Opini Masyarakat Terhadap Vaksinasi Nasional Menggunakan Metode Na√Øve Bayes dengan Seleksi Fitur TF-IDF (Bagian TF-IDF)\n",
    "\n",
    "Jadi dalam notebook ini, kita preprocessing dulu datasetnya. Lalu kita split datasetnya (80:20) dan hitung TF-IDF untuk fitur yang akan digunakan di Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d431733",
   "metadata": {},
   "source": [
    "## Import library yang dibutuhkan\n",
    "Kita butuh beberapa library biar semuanya lebih praktis dan perhitungannya efisien (ini yang paling penting). Tenang, semua library ini bukan \"fancy library\" kok. Ini semua library basic untuk text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518d66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np #untuk perhitungan array yang efisien\n",
    "import re #regex (untuk manipulasi karakter yang ada dalam string)\n",
    "import string \n",
    "from nltk.tokenize import word_tokenize #tokenisasi\n",
    "from nltk.probability import FreqDist #menghitung jumlah kemunculan tiap kata dalam dokumen\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory, ArrayDictionary #untuk mendapatkan list stopword\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory #stemming\n",
    "import ast #untuk mengubah string yang ada di file .csv menjadi list\n",
    "import seaborn as sns #visualisasi data\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "import matplotlib as mpl\n",
    "import csv\n",
    "from numpy.random import RandomState"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7b13cc",
   "metadata": {},
   "source": [
    "## Memuat training set\n",
    "Untuk melatih sistem, kita akan pake training set dulu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3050d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"dataset/dataset-tweet-vaksinasi-nasional.csv\"\n",
    "tweet_data = pd.read_csv(dataset)\n",
    "tweet_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c8492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "mpl.rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "f = sns.countplot(x='label', data=tweet_data)\n",
    "f.set_title(\"Sentiment Distribution\")\n",
    "f.set_xticklabels(['Negative', 'Positive'])\n",
    "plt.xlabel(\"\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c61dbc",
   "metadata": {},
   "source": [
    "## Preprocessing data\n",
    "Karena data dari twitter itu data mentah, kita preprocessing datanya dulu biar gampang diolah.\n",
    "\n",
    "- Case Folding (lowercasing)\n",
    "- Cleaning\n",
    "- Tokenization\n",
    "- Stopword removal\n",
    "- Converting/normalization\n",
    "- Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dad448",
   "metadata": {},
   "source": [
    "### Case Folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ded250e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercasing(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2762b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['casefolding'] = tweet_data['tweet'].apply(lowercasing)\n",
    "tweet_data['casefolding'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b6f721",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408b1be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(text):\n",
    "    # hapus karakter tab, new line, dan back slice\n",
    "    text = text.replace('\\\\t',\" \").replace('\\\\n',\" \").replace('\\\\u',\" \").replace('\\\\',\"\")\n",
    "    # hapus karakter non-ASCII (emotikon, huruf bahasa cina, dll)\n",
    "    text = text.encode('ascii', 'replace').decode('ascii')\n",
    "    # hapus mention dan link\n",
    "    text = ' '.join(re.sub(\"(@[A-Za-z0-9]+)|(\\w+:\\/\\/\\S+)\",\" \", text).split())\n",
    "    # hapus URL yang tidak komplit\n",
    "    text = text.replace(\"http://\", \" \").replace(\"https://\", \" \")\n",
    "    # hapus whitespace leading & trailing dan multiple whitespace jadi single whitespace\n",
    "    text = text.strip()\n",
    "    text = re.sub('\\s+',' ',text)\n",
    "    # hapus tanda baca\n",
    "    text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
    "    # hapus angka\n",
    "    cleaning_text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return cleaning_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c8bb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['cleaning'] = tweet_data['casefolding'].apply(cleaning)\n",
    "tweet_data['cleaning'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf5b1dd",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29121378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenisasi pake library NLTK (word_rokenize)\n",
    "def tokenization(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Distribusi frekuensi dari tiap kata dalam tiap tweet\n",
    "def freqDist_wrapper(text):\n",
    "    return FreqDist(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c41bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['tokenization'] = tweet_data['cleaning'].apply(tokenization)\n",
    "tweet_data['tokenization'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7745dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['tokens_freqdist'] = tweet_data['tokenization'].apply(freqDist_wrapper)\n",
    "tweet_data['tokens_freqdist'].head().apply(lambda x : x.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9a38de",
   "metadata": {},
   "source": [
    "### Stopword Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4c06a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dapetin stopword default dari library sastrawi\n",
    "stopword_factory = StopWordRemoverFactory().get_stop_words()\n",
    "# siapin stopword yang udah dibuat sendiri di .csv\n",
    "stopword_csv = \"csv files/stopwords.csv\"\n",
    "# pindahin stopword dari .csv ke list\n",
    "more_stopword = []\n",
    "with open(stopword_csv, newline='') as inputfile:\n",
    "    for row in csv.reader(inputfile):\n",
    "        more_stopword.append(row[0])\n",
    "\n",
    "# gabungin stopword default dan stopword yang udah dibuat sendiri\n",
    "stopword_data = stopword_factory + more_stopword\n",
    "list_stopwords = set(stopword_data)\n",
    "\n",
    "# hapus stopword yang ada ditiap tweet\n",
    "def stopwords_removal(words):\n",
    "    return [word for word in words if word not in list_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a176c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['stopword_removal'] = tweet_data['tokenization'].apply(stopwords_removal) \n",
    "tweet_data['stopword_removal'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce2f540",
   "metadata": {},
   "source": [
    "### Normalization/Converting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e44b917",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dapetin kamus normalisasi yang udah dibuat sendiri\n",
    "normalizad_word = pd.read_csv(\"csv files/normalization.csv\")\n",
    "# ubah csv ke dictionary\n",
    "normalizad_word_dict = {}\n",
    "for index, row in normalizad_word.iterrows():\n",
    "    if row[0] not in normalizad_word_dict:\n",
    "        normalizad_word_dict[row[0]] = row[1] \n",
    "\n",
    "# normalisasi kata yang ada didalam tweet\n",
    "def normalized_term(document):\n",
    "    return [normalizad_word_dict[term] if term in normalizad_word_dict else term for term in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55510160",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['converting'] = tweet_data['stopword_removal'].apply(normalized_term)\n",
    "tweet_data['converting'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0953d",
   "metadata": {},
   "source": [
    "### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dafdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buat stemmer dari sastrawi\n",
    "factory = StemmerFactory()\n",
    "stemmer = factory.create_stemmer()\n",
    "\n",
    "# stemming\n",
    "def stemmed_wrapper(term):\n",
    "    return stemmer.stem(term)\n",
    "\n",
    "# yang ada dibawah ini cuma untuk ngeprint output aja\n",
    "term_dict = {}\n",
    "\n",
    "for document in tweet_data['converting']:\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = ' '\n",
    "            \n",
    "print(len(term_dict))\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "for term in term_dict:\n",
    "    term_dict[term] = stemmed_wrapper(term)\n",
    "    print(term,\":\" ,term_dict[term])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877d4a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ini stemming juga tapi langsung ke dataframe\n",
    "def get_stemmed_term(document):\n",
    "    return [term_dict[term] for term in document]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ecd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data['stemming'] = tweet_data['converting'].apply(get_stemmed_term)\n",
    "tweet_data['stemming'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43776d67",
   "metadata": {},
   "source": [
    "Preprocessing selesai! Kita simpen dulu ke file .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629cae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_data.to_csv(\"csv files/preprocessing.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28476c64",
   "metadata": {},
   "source": [
    "### Split Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbd8893",
   "metadata": {},
   "outputs": [],
   "source": [
    "prepro_dataset = pd.read_csv(\"csv files/preprocessing.csv\", usecols=[\"label\", \"stemming\"])\n",
    "prepro_dataset.columns = [\"label\", \"tweet\"]\n",
    "prepro_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f77bbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus semua hashtag dibawah kalo mau training set yang baru atau cara cepetnya blok semua yang dibawah abis itu ctrl+/\n",
    "\n",
    "# train_set = prepro_dataset.sample(frac=0.8, random_state = RandomState())\n",
    "# train_set = train.reset_index(drop=True)\n",
    "# train_set.to_csv(\"csv files/train_80.csv\", index=False)\n",
    "# train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96681ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hapus semua hashtag dibawah kalo mau testing set yang baru atau cara cepetnya blok semua yang dibawah abis itu ctrl+/\n",
    "\n",
    "# test_set = prepro_dataset.loc[~prepro_dataset.index.isin(train.index)]\n",
    "# test_set = test.reset_index(drop=True)\n",
    "# test_set.to_csv(\"csv files/test_20.csv\", index=False)\n",
    "# test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c4ce12",
   "metadata": {},
   "source": [
    "## Features Extraction: TF-IDF\n",
    "Sebenernya fitur yang digunakan dalam Naive Bayes ini bisa aja menggunakan semua kata-kata yang ada didata. Tapi sekarang, kita pake fitur TF-IDF. Jadi bisa dibilang, TF-IDF ini tujuannya untuk mendapatkan kata-kata penting yang mengklasifikasikan sentimen dari sebuah tweet itu positif atau negatif. Bisa dibilang juga, kita mencari pola sentimen tweet dengan menggunakan TF-IDF."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8148c",
   "metadata": {},
   "source": [
    "### Import training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f07de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = \"csv files/train_80.csv\"\n",
    "train = pd.read_csv(train_csv)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a4ae853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil tiap tweet yang ada didalam dataset (tetep dalam format list)\n",
    "def convert_text_list(texts):\n",
    "    texts = ast.literal_eval(texts)\n",
    "    return [text for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e58573",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"tweet_list\"] = train[\"tweet\"].apply(convert_text_list)\n",
    "# menampilkan data ke 1 dalam bentuk list\n",
    "print(train[\"tweet_list\"][1])\n",
    "print(\"\\ntype : \", type(train[\"tweet_list\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aba970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitung TF yang ada dalam tiap tweet\n",
    "def calc_TF(document):\n",
    "    # Menghitung berapa kali kata itu muncul dalam tweet\n",
    "    TF_dict = {}\n",
    "    for term in document:\n",
    "        if term in TF_dict:\n",
    "            TF_dict[term] += 1\n",
    "        else:\n",
    "            TF_dict[term] = 1\n",
    "    # Hitung TF dari tiap kata\n",
    "    for term in TF_dict:\n",
    "        TF_dict[term] = TF_dict[term] / len(document)\n",
    "    return TF_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb39d50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"TF_dict\"] = train['tweet_list'].apply(calc_TF)\n",
    "train[\"TF_dict\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12917946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cek hasil TF dari index tertentu\n",
    "index = 0\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", \"TF\\n\")\n",
    "for key in train[\"TF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", train[\"TF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce614043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuma ngitung Document Frequency dari kata tersebut\n",
    "def calc_DF(tfDict):\n",
    "    count_DF = {}\n",
    "    # Perulangan melalui dictionary tf dari setiap dokumen dan tambahkan pasangan countDict (tem, doc)\n",
    "    for document in tfDict:\n",
    "        for term in document:\n",
    "            if term in count_DF:\n",
    "                count_DF[term] += 1\n",
    "            else:\n",
    "                count_DF[term] = 1\n",
    "    return count_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d76220",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_document = len(train)\n",
    "\n",
    "# Disini baru deh dihitung IDF-nya\n",
    "def calc_IDF(__n_document, __DF):\n",
    "    IDF_Dict = {}\n",
    "    for term in __DF:\n",
    "        IDF_Dict[term] = np.log(__n_document / (__DF[term] + 1))\n",
    "    return IDF_Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77355eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stores the idf dictionary\n",
    "IDF = calc_IDF(n_document, DF)\n",
    "print(IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfb889d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baru deh hitung TF-IDFnya\n",
    "def calc_TF_IDF(TF):\n",
    "    TF_IDF_Dict = {}\n",
    "    #For each word in the review, we multiply its tf and its idf.\n",
    "    for key in TF:\n",
    "        TF_IDF_Dict[key] = TF[key] * IDF[key]\n",
    "    return TF_IDF_Dict\n",
    "\n",
    "#Stores the TF-IDF Series\n",
    "train[\"TF-IDF_dict\"] = train[\"TF_dict\"].apply(calc_TF_IDF)\n",
    "print(train[\"TF-IDF_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d0830b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menampilkan TF dan TF-IDF dari tiap kata yang ada dalam tiap dokumen (contoh menampilkan pada index tertentu)\n",
    "index = 1\n",
    "\n",
    "print('%20s' % \"term\", \"\\t\", '%10s' % \"TF\", \"\\t\", '%20s' % \"TF-IDF\\n\")\n",
    "for key in train[\"TF-IDF_dict\"][index]:\n",
    "    print('%20s' % key, \"\\t\", train[\"TF_dict\"][index][key] ,\"\\t\" , train[\"TF-IDF_dict\"][index][key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c246838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagian ini emang susah untuk dimengerti, tapi intinya kita mau nambahin nilai TF-IDF dari tiap kata biar bisa dirangking\n",
    "\n",
    "# urutkan berdasarkan nilai dictionary DF\n",
    "sorted_DF = sorted(DF.items(), key=lambda kv: kv[1], reverse=True)\n",
    "\n",
    "# Create a list of unique words from sorted dictionay `sorted_DF`\n",
    "unique_term = [item[0] for item in sorted_DF]\n",
    "\n",
    "def calc_TF_IDF_Vec(__TF_IDF_Dict):\n",
    "    TF_IDF_vector = [0.0] * len(unique_term)\n",
    "\n",
    "    # For each unique word, if it is in the review, store its TF-IDF value.\n",
    "    for i, term in enumerate(unique_term):\n",
    "        if term in __TF_IDF_Dict:\n",
    "            TF_IDF_vector[i] = __TF_IDF_Dict[term]\n",
    "    return TF_IDF_vector\n",
    "\n",
    "train[\"TF_IDF_Vec\"] = train[\"TF-IDF_dict\"].apply(calc_TF_IDF_Vec)\n",
    "\n",
    "print(\"print first row matrix TF_IDF_Vec Series\\n\")\n",
    "print(train[\"TF_IDF_Vec\"][0])\n",
    "\n",
    "print(\"\\nmatrix size : \", len(train[\"TF_IDF_Vec\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82393a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mengkonversi Series ke List\n",
    "TF_IDF_Vec_List = np.array(train[\"TF_IDF_Vec\"].to_list())\n",
    "\n",
    "# Tambahkan elemen vektor yang ada di axix=0\n",
    "sums = TF_IDF_Vec_List.sum(axis=0)\n",
    "data = []\n",
    "for col, term in enumerate(unique_term):\n",
    "    data.append((term, sums[col]))\n",
    "\n",
    "# Rangking!\n",
    "ranking = pd.DataFrame(data, columns=['term', 'rank'])\n",
    "features_rank = ranking.sort_values('rank', ascending=False)\n",
    "features_rank.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4257063c",
   "metadata": {},
   "source": [
    "TF-IDF selesai! Kita simpan ke file .csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15dfe1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_rank.to_csv(\"csv files/tfidf_rank_train80.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
